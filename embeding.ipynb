{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "import openai # for calling the OpenAI API\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import tiktoken  # for counting tokens\n",
    "import os # for getting API token from env variable OPENAI_API_KEY\n",
    "from scipy import spatial  # for calculating vector similarities for search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de OpenAI\n",
    "openai.api_key = \"sk-proj-JEQe19qf1majBtkGMsubT3BlbkFJeGguQ7FTMjap1uVkFViu\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "MAX_TOKENS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = EMBEDDING_MODEL) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_from_txt_files(directory: str, model: str = EMBEDDING_MODEL, max_tokens: int = MAX_TOKENS) -> pd.DataFrame:\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                # Split the text into manageable chunks\n",
    "                token_count = num_tokens(text, model=model)\n",
    "                if token_count > max_tokens:\n",
    "                    print(f\"Warning: File {filename} exceeds max token limit and will be truncated.\")\n",
    "                    text = text[:max_tokens]\n",
    "                \n",
    "                response = openai.Embedding.create(model=model, input=text)\n",
    "                embedding = response['data'][0]['embedding']\n",
    "                \n",
    "                texts.append(text)\n",
    "                embeddings.append(embedding)\n",
    "    \n",
    "    df = pd.DataFrame({\"text\": texts, \"embedding\": embeddings})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: File Informes.txt exceeds max token limit and will be truncated.\n",
      "                                                text  \\\n",
      "0  Cálculo del consumo de combustible\\n\\nAl deter...   \n",
      "1  Los resultados se muestran en dos paneles: la ...   \n",
      "2   Primero, se selecciona la plantilla de inform...   \n",
      "3  En la pestaña de informes, se pueden crear inf...   \n",
      "\n",
      "                                           embedding  \n",
      "0  [-0.008142594248056412, 0.00439767399802804, 0...  \n",
      "1  [-0.016902418807148933, 0.006421457510441542, ...  \n",
      "2  [-0.02123759128153324, -0.013756769709289074, ...  \n",
      "3  [-0.0005416622734628618, 0.005904242396354675,...  \n"
     ]
    }
   ],
   "source": [
    "# Uso de la función\n",
    "directory = \"data\"\n",
    "df = create_embeddings_from_txt_files(directory)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para clasificar las strings por relevancia usando embeddings\n",
    "def strings_ranked_by_relatedness(query: str, df: pd.DataFrame, top_n: int = 5, threshold: float = 0.76) -> list[tuple[str, float]]:\n",
    "    query_embedding_response = openai.Embedding.create(model=EMBEDDING_MODEL, input=query)\n",
    "    query_embedding = query_embedding_response['data'][0]['embedding']\n",
    "    strings_and_relatednesses = [(row[\"text\"], 1 - spatial.distance.cosine(query_embedding, row[\"embedding\"])) for _, row in df.iterrows()]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    valid_sections = [(s, r) for s, r in strings_and_relatednesses if r >= threshold]\n",
    "    \n",
    "    return valid_sections[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el mensaje de consulta\n",
    "def query_message(query: str, df: pd.DataFrame, model: str, token_budget: int, threshold: float = 0.76) -> str:\n",
    "    valid_sections = strings_ranked_by_relatedness(query, df, threshold=threshold)\n",
    "    \n",
    "    if len(valid_sections) == 0:\n",
    "        return \"No cuento con esa información, por favor contactar a servicio a cliente.\"\n",
    "    \n",
    "    introduction = 'Use the below articles to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string, relatedness in valid_sections:\n",
    "        next_article = f'\\n\\nText section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if num_tokens(message + next_article + question, model=model) > token_budget:\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer la consulta a GPT\n",
    "def ask(query: str, df: pd.DataFrame, model: str = GPT_MODEL, token_budget: int = 4096 - 500, print_message: bool = False, threshold: float = 0.76) -> str:\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget, threshold=threshold)\n",
    "    if message == \"No cuento con esa información, por favor contactar a servicio a cliente.\":\n",
    "        return message\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You answer questions based on the provided articles.\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(model=model, messages=messages, temperature=0)\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_response(response: str) -> None:\n",
    "    formatted_response = response.replace(\"\\\\n\", \"\\n\")\n",
    "    formatted_response = formatted_response.replace('Wialon', 'Quamtum')\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los informes son herramientas que permiten analizar datos de seguimiento de una manera personalizada. Se pueden crear informes personalizados para analizar información específica, como en el caso de un informe de estacionamiento. Estos informes constan de tres secciones: contenido, ajustes y asignación. En el contenido del informe se incluye información relevante como el nombre del informe, grupos de unidades, intervalo de tiempo y tiempo de ejecución. En los ajustes del informe se definen aspectos como la agrupación de datos, las columnas a mostrar y la configuración del mapa. Una vez configurado el informe, se puede ejecutar para generar los resultados deseados, los cuales se mostrarán en el panel de resultados de informes. Los informes pueden exportarse a diferentes formatos, como PDF, Excel, HTML, XML o CSV, e incluso imprimirse.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "query = \"Hablame de los informes\"\n",
    "response = ask(query, df, model=GPT_MODEL, print_message=False, threshold=0.76)\n",
    "print(print_formatted_response(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
